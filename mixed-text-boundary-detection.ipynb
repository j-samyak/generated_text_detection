{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7379337,"sourceType":"datasetVersion","datasetId":4288371},{"sourceId":171230099,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T12:46:15.506333Z","iopub.execute_input":"2024-05-11T12:46:15.506658Z","iopub.status.idle":"2024-05-11T12:46:16.345444Z","shell.execute_reply.started":"2024-05-11T12:46:15.506630Z","shell.execute_reply":"2024-05-11T12:46:16.344390Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/semeval2024task8/subtaskA_dev_monolingual.jsonl\n/kaggle/input/semeval2024task8/subtaskA_train_monolingual.jsonl\n/kaggle/input/semeval2024task8/subtaskA_train_multilingual.jsonl\n/kaggle/input/semeval2024task8/subtaskB_train.jsonl\n/kaggle/input/semeval2024task8/subtaskA_dev_multilingual.jsonl\n/kaggle/input/semeval2024task8/subtaskC_dev.jsonl\n/kaggle/input/semeval2024task8/subtaskB_dev.jsonl\n/kaggle/input/semeval2024task8/subtaskC_train.jsonl\n/kaggle/input/mini-updated-code/config.json\n/kaggle/input/mini-updated-code/__results__.html\n/kaggle/input/mini-updated-code/tokenizer_config.json\n/kaggle/input/mini-updated-code/__notebook__.ipynb\n/kaggle/input/mini-updated-code/model.safetensors\n/kaggle/input/mini-updated-code/special_tokens_map.json\n/kaggle/input/mini-updated-code/__output__.json\n/kaggle/input/mini-updated-code/sentencepiece.bpe.model\n/kaggle/input/mini-updated-code/custom.css\n/kaggle/input/mini-updated-code/__results___files/__results___19_0.png\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# task c\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW, set_seed\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:47:20.437816Z","iopub.execute_input":"2024-05-11T12:47:20.438338Z","iopub.status.idle":"2024-05-11T12:47:25.670119Z","shell.execute_reply.started":"2024-05-11T12:47:20.438306Z","shell.execute_reply":"2024-05-11T12:47:25.669306Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"random_seed = 0\nset_seed(random_seed)\ntrain_path =  '/kaggle/input/semeval2024task8/subtaskC_train.jsonl'\ntest_path =  '/kaggle/input/semeval2024task8/subtaskC_dev.jsonl'\n\ntrain_df = pd.read_json(train_path, lines=True)\neval_df = pd.read_json(test_path, lines=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:47:38.093419Z","iopub.execute_input":"2024-05-11T12:47:38.093949Z","iopub.status.idle":"2024-05-11T12:47:47.522379Z","shell.execute_reply.started":"2024-05-11T12:47:38.093909Z","shell.execute_reply":"2024-05-11T12:47:47.521280Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-11 12:47:39.695898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-11 12:47:39.695993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-11 12:47:39.813912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df['new_label'] = train_df['label']","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:48:07.595583Z","iopub.execute_input":"2024-05-11T12:48:07.596401Z","iopub.status.idle":"2024-05-11T12:48:07.601550Z","shell.execute_reply.started":"2024-05-11T12:48:07.596368Z","shell.execute_reply":"2024-05-11T12:48:07.600486Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:48:14.465907Z","iopub.execute_input":"2024-05-11T12:48:14.466250Z","iopub.status.idle":"2024-05-11T12:48:14.478930Z","shell.execute_reply.started":"2024-05-11T12:48:14.466225Z","shell.execute_reply":"2024-05-11T12:48:14.477905Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                        id  \\\n0     5df7a120-f5ff-4881-8624-058f8a2fee5a   \n1     063f9429-b94b-4e86-bd4c-c48e9bf85850   \n2     2759de87-3708-4f81-9e5f-ed132c292d2f   \n3     25ef7554-db90-4212-b218-06679b25efb8   \n4     8b51f390-6552-4982-b5d4-1b165e598df1   \n...                                    ...   \n3644  35a52bbb-c5d8-4446-8e5c-b4521d895196   \n3645  e8941b7e-e562-4c91-b051-678d0b4a885a   \n3646  1faa5671-0ec6-4b9a-b539-c7381d09b518   \n3647  e0078728-7e68-467b-b843-0572211ad812   \n3648  436fa3e9-e2cf-4ac9-b750-cd5e5402a1a8   \n\n                                                   text  label  new_label  \n0     - Strengths:\\r\\n* Outperforms ALIGN in supervi...     88         88  \n1     This paper addresses the problem of disambigua...    286        286  \n2     - Strengths:\\r\\nGood ideas, simple neural lear...     62         62  \n3     This paper presents several weakly supervised ...     35         35  \n4     This paper describes a model for cross-lingual...     98         98  \n...                                                 ...    ...        ...  \n3644  The paper presents a novel method for reducing...      4          4  \n3645  This paper addresses to reduce test-time compu...     10         10  \n3646  I do need to see the results in a clear table....     37         37  \n3647  This paper explores a new quantization method ...     54         54  \n3648  .The paper presents a novel method for reducin...      0          0  \n\n[3649 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>new_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5df7a120-f5ff-4881-8624-058f8a2fee5a</td>\n      <td>- Strengths:\\r\\n* Outperforms ALIGN in supervi...</td>\n      <td>88</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>063f9429-b94b-4e86-bd4c-c48e9bf85850</td>\n      <td>This paper addresses the problem of disambigua...</td>\n      <td>286</td>\n      <td>286</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2759de87-3708-4f81-9e5f-ed132c292d2f</td>\n      <td>- Strengths:\\r\\nGood ideas, simple neural lear...</td>\n      <td>62</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25ef7554-db90-4212-b218-06679b25efb8</td>\n      <td>This paper presents several weakly supervised ...</td>\n      <td>35</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8b51f390-6552-4982-b5d4-1b165e598df1</td>\n      <td>This paper describes a model for cross-lingual...</td>\n      <td>98</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3644</th>\n      <td>35a52bbb-c5d8-4446-8e5c-b4521d895196</td>\n      <td>The paper presents a novel method for reducing...</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3645</th>\n      <td>e8941b7e-e562-4c91-b051-678d0b4a885a</td>\n      <td>This paper addresses to reduce test-time compu...</td>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3646</th>\n      <td>1faa5671-0ec6-4b9a-b539-c7381d09b518</td>\n      <td>I do need to see the results in a clear table....</td>\n      <td>37</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>3647</th>\n      <td>e0078728-7e68-467b-b843-0572211ad812</td>\n      <td>This paper explores a new quantization method ...</td>\n      <td>54</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>3648</th>\n      <td>436fa3e9-e2cf-4ac9-b750-cd5e5402a1a8</td>\n      <td>.The paper presents a novel method for reducin...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3649 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Load tokenizer (replace with your chosen model)\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:48:33.304481Z","iopub.execute_input":"2024-05-11T12:48:33.305139Z","iopub.status.idle":"2024-05-11T12:48:36.099864Z","shell.execute_reply.started":"2024-05-11T12:48:33.305107Z","shell.execute_reply":"2024-05-11T12:48:36.099029Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edae558fc9684566b93e157f8be70e8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e529cde05c140c9a974f42099120d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a31e59a3864bf38d08b5c8ef70d717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b4cbb446a44552a6c13757a63e11d3"}},"metadata":{}}]},{"cell_type":"code","source":"a = []\nb = []\nprint(a)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:48:41.247842Z","iopub.execute_input":"2024-05-11T12:48:41.248231Z","iopub.status.idle":"2024-05-11T12:48:41.253041Z","shell.execute_reply.started":"2024-05-11T12:48:41.248203Z","shell.execute_reply":"2024-05-11T12:48:41.252144Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(len(train_df)):\n    mapp = {x : tokenizer.encode(x, add_special_tokens=False) for x in train_df['text'][i].split()}\n    sum_a = 0\n    sum_b = 0\n    label_index = train_df['new_label'][i]\n    for j,x in enumerate(mapp):\n        sum_a += len(mapp[x])\n        if(j <= label_index):\n            sum_b += len(mapp[x])\n    a.append(sum_a)\n    b.append(sum_b)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:48:43.237699Z","iopub.execute_input":"2024-05-11T12:48:43.238090Z","iopub.status.idle":"2024-05-11T12:49:31.148376Z","shell.execute_reply.started":"2024-05-11T12:48:43.238052Z","shell.execute_reply":"2024-05-11T12:49:31.147606Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"c = b\nc = [x if x <= 512 else 512 for x in c]","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:49:35.695942Z","iopub.execute_input":"2024-05-11T12:49:35.696277Z","iopub.status.idle":"2024-05-11T12:49:35.701266Z","shell.execute_reply.started":"2024-05-11T12:49:35.696253Z","shell.execute_reply":"2024-05-11T12:49:35.700126Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df['label'] = c","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:49:39.319283Z","iopub.execute_input":"2024-05-11T12:49:39.319669Z","iopub.status.idle":"2024-05-11T12:49:39.325702Z","shell.execute_reply.started":"2024-05-11T12:49:39.319644Z","shell.execute_reply":"2024-05-11T12:49:39.324837Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_df['label'].max()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:49:40.783658Z","iopub.execute_input":"2024-05-11T12:49:40.784035Z","iopub.status.idle":"2024-05-11T12:49:40.792002Z","shell.execute_reply.started":"2024-05-11T12:49:40.784004Z","shell.execute_reply":"2024-05-11T12:49:40.791097Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:49:43.013402Z","iopub.execute_input":"2024-05-11T12:49:43.013749Z","iopub.status.idle":"2024-05-11T12:49:43.026446Z","shell.execute_reply.started":"2024-05-11T12:49:43.013721Z","shell.execute_reply":"2024-05-11T12:49:43.025437Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                        id  \\\n0     5df7a120-f5ff-4881-8624-058f8a2fee5a   \n1     063f9429-b94b-4e86-bd4c-c48e9bf85850   \n2     2759de87-3708-4f81-9e5f-ed132c292d2f   \n3     25ef7554-db90-4212-b218-06679b25efb8   \n4     8b51f390-6552-4982-b5d4-1b165e598df1   \n...                                    ...   \n3644  35a52bbb-c5d8-4446-8e5c-b4521d895196   \n3645  e8941b7e-e562-4c91-b051-678d0b4a885a   \n3646  1faa5671-0ec6-4b9a-b539-c7381d09b518   \n3647  e0078728-7e68-467b-b843-0572211ad812   \n3648  436fa3e9-e2cf-4ac9-b750-cd5e5402a1a8   \n\n                                                   text  label  new_label  \n0     - Strengths:\\r\\n* Outperforms ALIGN in supervi...    137         88  \n1     This paper addresses the problem of disambigua...    455        286  \n2     - Strengths:\\r\\nGood ideas, simple neural lear...    102         62  \n3     This paper presents several weakly supervised ...     49         35  \n4     This paper describes a model for cross-lingual...    170         98  \n...                                                 ...    ...        ...  \n3644  The paper presents a novel method for reducing...      6          4  \n3645  This paper addresses to reduce test-time compu...     21         10  \n3646  I do need to see the results in a clear table....     52         37  \n3647  This paper explores a new quantization method ...     78         54  \n3648  .The paper presents a novel method for reducin...      3          0  \n\n[3649 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>new_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5df7a120-f5ff-4881-8624-058f8a2fee5a</td>\n      <td>- Strengths:\\r\\n* Outperforms ALIGN in supervi...</td>\n      <td>137</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>063f9429-b94b-4e86-bd4c-c48e9bf85850</td>\n      <td>This paper addresses the problem of disambigua...</td>\n      <td>455</td>\n      <td>286</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2759de87-3708-4f81-9e5f-ed132c292d2f</td>\n      <td>- Strengths:\\r\\nGood ideas, simple neural lear...</td>\n      <td>102</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25ef7554-db90-4212-b218-06679b25efb8</td>\n      <td>This paper presents several weakly supervised ...</td>\n      <td>49</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8b51f390-6552-4982-b5d4-1b165e598df1</td>\n      <td>This paper describes a model for cross-lingual...</td>\n      <td>170</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3644</th>\n      <td>35a52bbb-c5d8-4446-8e5c-b4521d895196</td>\n      <td>The paper presents a novel method for reducing...</td>\n      <td>6</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3645</th>\n      <td>e8941b7e-e562-4c91-b051-678d0b4a885a</td>\n      <td>This paper addresses to reduce test-time compu...</td>\n      <td>21</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3646</th>\n      <td>1faa5671-0ec6-4b9a-b539-c7381d09b518</td>\n      <td>I do need to see the results in a clear table....</td>\n      <td>52</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>3647</th>\n      <td>e0078728-7e68-467b-b843-0572211ad812</td>\n      <td>This paper explores a new quantization method ...</td>\n      <td>78</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>3648</th>\n      <td>436fa3e9-e2cf-4ac9-b750-cd5e5402a1a8</td>\n      <td>.The paper presents a novel method for reducin...</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3649 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:49:48.519382Z","iopub.execute_input":"2024-05-11T12:49:48.520121Z","iopub.status.idle":"2024-05-11T12:49:48.584803Z","shell.execute_reply.started":"2024-05-11T12:49:48.520088Z","shell.execute_reply":"2024-05-11T12:49:48.583805Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaConfig, AdamW\n\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\nconfig = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\n\n# Define the path to your partially trained model\nmodel_path = \"/kaggle/input/mini-updated-code\"\n\n# Load the partially trained model\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_path, config=config)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:49:51.356907Z","iopub.execute_input":"2024-05-11T12:49:51.357773Z","iopub.status.idle":"2024-05-11T12:50:01.144906Z","shell.execute_reply.started":"2024-05-11T12:49:51.357738Z","shell.execute_reply":"2024-05-11T12:50:01.143928Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:05.078701Z","iopub.execute_input":"2024-05-11T12:50:05.079124Z","iopub.status.idle":"2024-05-11T12:50:05.087628Z","shell.execute_reply.started":"2024-05-11T12:50:05.079089Z","shell.execute_reply":"2024-05-11T12:50:05.086626Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"XLMRobertaForSequenceClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): XLMRobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import XLMRobertaForSequenceClassification, XLMRobertaConfig\n# Define the new model configuration\nnew_model_name = \"xlm-roberta-base\"\nnew_config = XLMRobertaConfig.from_pretrained(new_model_name)\nnew_config.num_labels = 512\nnew_model = XLMRobertaForSequenceClassification(config=new_config)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:07.542248Z","iopub.execute_input":"2024-05-11T12:50:07.542565Z","iopub.status.idle":"2024-05-11T12:50:12.806588Z","shell.execute_reply.started":"2024-05-11T12:50:07.542540Z","shell.execute_reply":"2024-05-11T12:50:12.805796Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"new_model","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:15.800782Z","iopub.execute_input":"2024-05-11T12:50:15.801183Z","iopub.status.idle":"2024-05-11T12:50:15.809047Z","shell.execute_reply.started":"2024-05-11T12:50:15.801151Z","shell.execute_reply":"2024-05-11T12:50:15.807947Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"XLMRobertaForSequenceClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): XLMRobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=512, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Copy the parameters from the pretrained model to the new model, except the classifier layer\npretrained_state_dict = model.state_dict()\nnew_model_state_dict = new_model.state_dict()\n\n# Filter out the classifier layer parameters\nclassifier_params = {k: v for k, v in pretrained_state_dict.items() if 'classifier' in k}\n\n# Update the new model's state dict with the parameters of the pretrained model, except the classifier layer\nnew_model_state_dict.update({k: v for k, v in pretrained_state_dict.items() if k not in classifier_params})\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:19.441554Z","iopub.execute_input":"2024-05-11T12:50:19.442234Z","iopub.status.idle":"2024-05-11T12:50:19.452194Z","shell.execute_reply.started":"2024-05-11T12:50:19.442201Z","shell.execute_reply":"2024-05-11T12:50:19.451303Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Load the updated state dict into the new model\nnew_model.load_state_dict(new_model_state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:21.863708Z","iopub.execute_input":"2024-05-11T12:50:21.864533Z","iopub.status.idle":"2024-05-11T12:50:22.135246Z","shell.execute_reply.started":"2024-05-11T12:50:21.864500Z","shell.execute_reply":"2024-05-11T12:50:22.134371Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# # Freeze all parameters except those of the classifier layer\n# for name, param in new_model.named_parameters():\n#     if 'classifier' not in name:  # If the parameter does not belong to the classifier layer\n#         param.requires_grad = False\n\n# # Verify that parameters are frozen as intended\n# for name, param in new_model.named_parameters():\n#     print(f'{name}: {param.requires_grad}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = new_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:36.565374Z","iopub.execute_input":"2024-05-11T12:50:36.566136Z","iopub.status.idle":"2024-05-11T12:50:36.871402Z","shell.execute_reply.started":"2024-05-11T12:50:36.566100Z","shell.execute_reply":"2024-05-11T12:50:36.870627Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Tokenize text data from DataFrame\ntrain_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True)\neval_encodings = tokenizer(eval_df['text'].tolist(), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:40.892721Z","iopub.execute_input":"2024-05-11T12:50:40.893073Z","iopub.status.idle":"2024-05-11T12:50:50.202714Z","shell.execute_reply.started":"2024-05-11T12:50:40.893044Z","shell.execute_reply":"2024-05-11T12:50:50.201941Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx]).to(device)\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:52.149195Z","iopub.execute_input":"2024-05-11T12:50:52.150004Z","iopub.status.idle":"2024-05-11T12:50:52.156196Z","shell.execute_reply.started":"2024-05-11T12:50:52.149973Z","shell.execute_reply":"2024-05-11T12:50:52.155169Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class CustomDatasetTest(Dataset):\n    def __init__(self, encodings, labels,text):\n        self.encodings = encodings\n        self.labels = labels\n        self.text = text\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx]).to(device)\n        item['text'] = self.text[idx]\n        return item\n        \n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:53.575977Z","iopub.execute_input":"2024-05-11T12:50:53.576321Z","iopub.status.idle":"2024-05-11T12:50:53.583112Z","shell.execute_reply.started":"2024-05-11T12:50:53.576294Z","shell.execute_reply":"2024-05-11T12:50:53.582086Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Define training parameters\nbatch_size = 8\nlearning_rate = 2.0e-5\nnum_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:50:55.640440Z","iopub.execute_input":"2024-05-11T12:50:55.641279Z","iopub.status.idle":"2024-05-11T12:50:55.645544Z","shell.execute_reply.started":"2024-05-11T12:50:55.641245Z","shell.execute_reply":"2024-05-11T12:50:55.644513Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\neval_dataset = CustomDatasetTest(eval_encodings, eval_df['label'].tolist(),eval_df['text'].tolist())\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\neval_loader = DataLoader(eval_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:51:03.603661Z","iopub.execute_input":"2024-05-11T12:51:03.604519Z","iopub.status.idle":"2024-05-11T12:51:03.610403Z","shell.execute_reply.started":"2024-05-11T12:51:03.604484Z","shell.execute_reply":"2024-05-11T12:51:03.609378Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=1.0e-3)\ncriterion1 = nn.CrossEntropyLoss()\ncriterion2 = nn.MSELoss()\ncriterion3 = nn.L1Loss()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:27:43.025792Z","iopub.execute_input":"2024-05-11T13:27:43.026430Z","iopub.status.idle":"2024-05-11T13:27:43.035854Z","shell.execute_reply.started":"2024-05-11T13:27:43.026399Z","shell.execute_reply":"2024-05-11T13:27:43.034820Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# for name, param in new_model.named_parameters():\n#         param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:51:13.950602Z","iopub.execute_input":"2024-05-11T12:51:13.950965Z","iopub.status.idle":"2024-05-11T12:51:13.956009Z","shell.execute_reply.started":"2024-05-11T12:51:13.950934Z","shell.execute_reply":"2024-05-11T12:51:13.954908Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Verify that parameters are frozen as intended\nfor name, param in new_model.named_parameters():\n    print(f'{name}: {param.requires_grad}')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:51:17.701277Z","iopub.execute_input":"2024-05-11T12:51:17.701979Z","iopub.status.idle":"2024-05-11T12:51:17.709791Z","shell.execute_reply.started":"2024-05-11T12:51:17.701946Z","shell.execute_reply":"2024-05-11T12:51:17.708833Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"roberta.embeddings.word_embeddings.weight: True\nroberta.embeddings.position_embeddings.weight: True\nroberta.embeddings.token_type_embeddings.weight: True\nroberta.embeddings.LayerNorm.weight: True\nroberta.embeddings.LayerNorm.bias: True\nroberta.encoder.layer.0.attention.self.query.weight: True\nroberta.encoder.layer.0.attention.self.query.bias: True\nroberta.encoder.layer.0.attention.self.key.weight: True\nroberta.encoder.layer.0.attention.self.key.bias: True\nroberta.encoder.layer.0.attention.self.value.weight: True\nroberta.encoder.layer.0.attention.self.value.bias: True\nroberta.encoder.layer.0.attention.output.dense.weight: True\nroberta.encoder.layer.0.attention.output.dense.bias: True\nroberta.encoder.layer.0.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.0.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.0.intermediate.dense.weight: True\nroberta.encoder.layer.0.intermediate.dense.bias: True\nroberta.encoder.layer.0.output.dense.weight: True\nroberta.encoder.layer.0.output.dense.bias: True\nroberta.encoder.layer.0.output.LayerNorm.weight: True\nroberta.encoder.layer.0.output.LayerNorm.bias: True\nroberta.encoder.layer.1.attention.self.query.weight: True\nroberta.encoder.layer.1.attention.self.query.bias: True\nroberta.encoder.layer.1.attention.self.key.weight: True\nroberta.encoder.layer.1.attention.self.key.bias: True\nroberta.encoder.layer.1.attention.self.value.weight: True\nroberta.encoder.layer.1.attention.self.value.bias: True\nroberta.encoder.layer.1.attention.output.dense.weight: True\nroberta.encoder.layer.1.attention.output.dense.bias: True\nroberta.encoder.layer.1.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.1.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.1.intermediate.dense.weight: True\nroberta.encoder.layer.1.intermediate.dense.bias: True\nroberta.encoder.layer.1.output.dense.weight: True\nroberta.encoder.layer.1.output.dense.bias: True\nroberta.encoder.layer.1.output.LayerNorm.weight: True\nroberta.encoder.layer.1.output.LayerNorm.bias: True\nroberta.encoder.layer.2.attention.self.query.weight: True\nroberta.encoder.layer.2.attention.self.query.bias: True\nroberta.encoder.layer.2.attention.self.key.weight: True\nroberta.encoder.layer.2.attention.self.key.bias: True\nroberta.encoder.layer.2.attention.self.value.weight: True\nroberta.encoder.layer.2.attention.self.value.bias: True\nroberta.encoder.layer.2.attention.output.dense.weight: True\nroberta.encoder.layer.2.attention.output.dense.bias: True\nroberta.encoder.layer.2.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.2.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.2.intermediate.dense.weight: True\nroberta.encoder.layer.2.intermediate.dense.bias: True\nroberta.encoder.layer.2.output.dense.weight: True\nroberta.encoder.layer.2.output.dense.bias: True\nroberta.encoder.layer.2.output.LayerNorm.weight: True\nroberta.encoder.layer.2.output.LayerNorm.bias: True\nroberta.encoder.layer.3.attention.self.query.weight: True\nroberta.encoder.layer.3.attention.self.query.bias: True\nroberta.encoder.layer.3.attention.self.key.weight: True\nroberta.encoder.layer.3.attention.self.key.bias: True\nroberta.encoder.layer.3.attention.self.value.weight: True\nroberta.encoder.layer.3.attention.self.value.bias: True\nroberta.encoder.layer.3.attention.output.dense.weight: True\nroberta.encoder.layer.3.attention.output.dense.bias: True\nroberta.encoder.layer.3.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.3.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.3.intermediate.dense.weight: True\nroberta.encoder.layer.3.intermediate.dense.bias: True\nroberta.encoder.layer.3.output.dense.weight: True\nroberta.encoder.layer.3.output.dense.bias: True\nroberta.encoder.layer.3.output.LayerNorm.weight: True\nroberta.encoder.layer.3.output.LayerNorm.bias: True\nroberta.encoder.layer.4.attention.self.query.weight: True\nroberta.encoder.layer.4.attention.self.query.bias: True\nroberta.encoder.layer.4.attention.self.key.weight: True\nroberta.encoder.layer.4.attention.self.key.bias: True\nroberta.encoder.layer.4.attention.self.value.weight: True\nroberta.encoder.layer.4.attention.self.value.bias: True\nroberta.encoder.layer.4.attention.output.dense.weight: True\nroberta.encoder.layer.4.attention.output.dense.bias: True\nroberta.encoder.layer.4.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.4.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.4.intermediate.dense.weight: True\nroberta.encoder.layer.4.intermediate.dense.bias: True\nroberta.encoder.layer.4.output.dense.weight: True\nroberta.encoder.layer.4.output.dense.bias: True\nroberta.encoder.layer.4.output.LayerNorm.weight: True\nroberta.encoder.layer.4.output.LayerNorm.bias: True\nroberta.encoder.layer.5.attention.self.query.weight: True\nroberta.encoder.layer.5.attention.self.query.bias: True\nroberta.encoder.layer.5.attention.self.key.weight: True\nroberta.encoder.layer.5.attention.self.key.bias: True\nroberta.encoder.layer.5.attention.self.value.weight: True\nroberta.encoder.layer.5.attention.self.value.bias: True\nroberta.encoder.layer.5.attention.output.dense.weight: True\nroberta.encoder.layer.5.attention.output.dense.bias: True\nroberta.encoder.layer.5.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.5.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.5.intermediate.dense.weight: True\nroberta.encoder.layer.5.intermediate.dense.bias: True\nroberta.encoder.layer.5.output.dense.weight: True\nroberta.encoder.layer.5.output.dense.bias: True\nroberta.encoder.layer.5.output.LayerNorm.weight: True\nroberta.encoder.layer.5.output.LayerNorm.bias: True\nroberta.encoder.layer.6.attention.self.query.weight: True\nroberta.encoder.layer.6.attention.self.query.bias: True\nroberta.encoder.layer.6.attention.self.key.weight: True\nroberta.encoder.layer.6.attention.self.key.bias: True\nroberta.encoder.layer.6.attention.self.value.weight: True\nroberta.encoder.layer.6.attention.self.value.bias: True\nroberta.encoder.layer.6.attention.output.dense.weight: True\nroberta.encoder.layer.6.attention.output.dense.bias: True\nroberta.encoder.layer.6.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.6.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.6.intermediate.dense.weight: True\nroberta.encoder.layer.6.intermediate.dense.bias: True\nroberta.encoder.layer.6.output.dense.weight: True\nroberta.encoder.layer.6.output.dense.bias: True\nroberta.encoder.layer.6.output.LayerNorm.weight: True\nroberta.encoder.layer.6.output.LayerNorm.bias: True\nroberta.encoder.layer.7.attention.self.query.weight: True\nroberta.encoder.layer.7.attention.self.query.bias: True\nroberta.encoder.layer.7.attention.self.key.weight: True\nroberta.encoder.layer.7.attention.self.key.bias: True\nroberta.encoder.layer.7.attention.self.value.weight: True\nroberta.encoder.layer.7.attention.self.value.bias: True\nroberta.encoder.layer.7.attention.output.dense.weight: True\nroberta.encoder.layer.7.attention.output.dense.bias: True\nroberta.encoder.layer.7.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.7.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.7.intermediate.dense.weight: True\nroberta.encoder.layer.7.intermediate.dense.bias: True\nroberta.encoder.layer.7.output.dense.weight: True\nroberta.encoder.layer.7.output.dense.bias: True\nroberta.encoder.layer.7.output.LayerNorm.weight: True\nroberta.encoder.layer.7.output.LayerNorm.bias: True\nroberta.encoder.layer.8.attention.self.query.weight: True\nroberta.encoder.layer.8.attention.self.query.bias: True\nroberta.encoder.layer.8.attention.self.key.weight: True\nroberta.encoder.layer.8.attention.self.key.bias: True\nroberta.encoder.layer.8.attention.self.value.weight: True\nroberta.encoder.layer.8.attention.self.value.bias: True\nroberta.encoder.layer.8.attention.output.dense.weight: True\nroberta.encoder.layer.8.attention.output.dense.bias: True\nroberta.encoder.layer.8.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.8.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.8.intermediate.dense.weight: True\nroberta.encoder.layer.8.intermediate.dense.bias: True\nroberta.encoder.layer.8.output.dense.weight: True\nroberta.encoder.layer.8.output.dense.bias: True\nroberta.encoder.layer.8.output.LayerNorm.weight: True\nroberta.encoder.layer.8.output.LayerNorm.bias: True\nroberta.encoder.layer.9.attention.self.query.weight: True\nroberta.encoder.layer.9.attention.self.query.bias: True\nroberta.encoder.layer.9.attention.self.key.weight: True\nroberta.encoder.layer.9.attention.self.key.bias: True\nroberta.encoder.layer.9.attention.self.value.weight: True\nroberta.encoder.layer.9.attention.self.value.bias: True\nroberta.encoder.layer.9.attention.output.dense.weight: True\nroberta.encoder.layer.9.attention.output.dense.bias: True\nroberta.encoder.layer.9.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.9.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.9.intermediate.dense.weight: True\nroberta.encoder.layer.9.intermediate.dense.bias: True\nroberta.encoder.layer.9.output.dense.weight: True\nroberta.encoder.layer.9.output.dense.bias: True\nroberta.encoder.layer.9.output.LayerNorm.weight: True\nroberta.encoder.layer.9.output.LayerNorm.bias: True\nroberta.encoder.layer.10.attention.self.query.weight: True\nroberta.encoder.layer.10.attention.self.query.bias: True\nroberta.encoder.layer.10.attention.self.key.weight: True\nroberta.encoder.layer.10.attention.self.key.bias: True\nroberta.encoder.layer.10.attention.self.value.weight: True\nroberta.encoder.layer.10.attention.self.value.bias: True\nroberta.encoder.layer.10.attention.output.dense.weight: True\nroberta.encoder.layer.10.attention.output.dense.bias: True\nroberta.encoder.layer.10.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.10.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.10.intermediate.dense.weight: True\nroberta.encoder.layer.10.intermediate.dense.bias: True\nroberta.encoder.layer.10.output.dense.weight: True\nroberta.encoder.layer.10.output.dense.bias: True\nroberta.encoder.layer.10.output.LayerNorm.weight: True\nroberta.encoder.layer.10.output.LayerNorm.bias: True\nroberta.encoder.layer.11.attention.self.query.weight: True\nroberta.encoder.layer.11.attention.self.query.bias: True\nroberta.encoder.layer.11.attention.self.key.weight: True\nroberta.encoder.layer.11.attention.self.key.bias: True\nroberta.encoder.layer.11.attention.self.value.weight: True\nroberta.encoder.layer.11.attention.self.value.bias: True\nroberta.encoder.layer.11.attention.output.dense.weight: True\nroberta.encoder.layer.11.attention.output.dense.bias: True\nroberta.encoder.layer.11.attention.output.LayerNorm.weight: True\nroberta.encoder.layer.11.attention.output.LayerNorm.bias: True\nroberta.encoder.layer.11.intermediate.dense.weight: True\nroberta.encoder.layer.11.intermediate.dense.bias: True\nroberta.encoder.layer.11.output.dense.weight: True\nroberta.encoder.layer.11.output.dense.bias: True\nroberta.encoder.layer.11.output.LayerNorm.weight: True\nroberta.encoder.layer.11.output.LayerNorm.bias: True\nclassifier.dense.weight: True\nclassifier.dense.bias: True\nclassifier.out_proj.weight: True\nclassifier.out_proj.bias: True\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loss = []\ntest_loss = []","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:51:24.810383Z","iopub.execute_input":"2024-05-11T12:51:24.810712Z","iopub.status.idle":"2024-05-11T12:51:24.814840Z","shell.execute_reply.started":"2024-05-11T12:51:24.810685Z","shell.execute_reply":"2024-05-11T12:51:24.813948Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Function for training\ndef train(model, optimizer, criterion, dataloader, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for batch in tqdm(train_loader, leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            labels = labels.to(torch.float32)\n\n            optimizer.zero_grad()\n\n            outputs = new_model(input_ids, attention_mask=attention_mask)\n            prediction = torch.argmax(outputs.logits, dim=1).to(torch.float32)\n            prediction.requires_grad_(True)\n            loss =torch.sqrt(criterion2(prediction,labels))\n\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * input_ids.size(0)\n\n            del input_ids, attention_mask, labels, outputs\n            torch.cuda.empty_cache()\n            \n        epoch_loss = running_loss / len(dataloader.dataset)\n        train_loss.append(epoch_loss)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:27:53.400997Z","iopub.execute_input":"2024-05-11T13:27:53.401350Z","iopub.status.idle":"2024-05-11T13:27:53.410197Z","shell.execute_reply.started":"2024-05-11T13:27:53.401322Z","shell.execute_reply":"2024-05-11T13:27:53.409309Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def find_position(text, encoded_pos,tokenizer):\n    mapp = {x : tokenizer.encode(x, add_special_tokens=False) for x in text}\n    sumi = 0\n    for j,x in enumerate(mapp):\n        sumi += len(mapp[x])\n        if(sumi >= encoded_pos.item()):\n            return j\n    return len(mapp)\n\n# Function for evaluation\ndef evaluate(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n            text = batch['text']\n            \n            outputs = model(input_ids, attention_mask=attention_mask)\n            encoded_pos = torch.argmax(outputs.logits, dim=1)\n            for i in range(len(text)):\n                pos = find_position(text[i],encoded_pos[i],tokenizer)\n                all_preds.append(pos)\n            all_labels.extend(labels.cpu().numpy())\n            \n            del input_ids, attention_mask, labels, outputs\n            torch.cuda.empty_cache()\n    return all_preds,all_labels","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:27:56.153060Z","iopub.execute_input":"2024-05-11T13:27:56.153858Z","iopub.status.idle":"2024-05-11T13:27:56.162826Z","shell.execute_reply.started":"2024-05-11T13:27:56.153825Z","shell.execute_reply":"2024-05-11T13:27:56.161848Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\nfor epoch in range(num_epochs):\n    train(new_model, optimizer, criterion2, train_loader, 1)\n    all_preds,all_labels = evaluate(new_model, eval_loader)\n    all_preds = torch.tensor(all_preds).to(torch.float32)\n    all_labels = torch.tensor(all_labels).to(torch.float32)\n    loss =torch.sqrt(criterion2(all_preds,all_labels))\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Test Loss: {loss:.4f}\")\n    test_loss.append(loss)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:51:52.383294Z","iopub.execute_input":"2024-05-11T12:51:52.384122Z","iopub.status.idle":"2024-05-11T13:09:34.722260Z","shell.execute_reply.started":"2024-05-11T12:51:52.384090Z","shell.execute_reply":"2024-05-11T13:09:34.721175Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 185.2274\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:47<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 185.3130\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:47<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 185.8081\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:47<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 185.1235\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:47<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 184.1484\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:47<00:00,  1.34it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in new_model.named_parameters():\n    if 'classifier' not in name:  # If the parameter does not belong to the classifier layer\n        param.requires_grad = False\n\n# Verify that parameters are frozen as intended\nfor name, param in new_model.named_parameters():\n    print(f'{name}: {param.requires_grad}')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:28:03.770102Z","iopub.execute_input":"2024-05-11T13:28:03.770436Z","iopub.status.idle":"2024-05-11T13:28:03.779797Z","shell.execute_reply.started":"2024-05-11T13:28:03.770410Z","shell.execute_reply":"2024-05-11T13:28:03.778885Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"roberta.embeddings.word_embeddings.weight: False\nroberta.embeddings.position_embeddings.weight: False\nroberta.embeddings.token_type_embeddings.weight: False\nroberta.embeddings.LayerNorm.weight: False\nroberta.embeddings.LayerNorm.bias: False\nroberta.encoder.layer.0.attention.self.query.weight: False\nroberta.encoder.layer.0.attention.self.query.bias: False\nroberta.encoder.layer.0.attention.self.key.weight: False\nroberta.encoder.layer.0.attention.self.key.bias: False\nroberta.encoder.layer.0.attention.self.value.weight: False\nroberta.encoder.layer.0.attention.self.value.bias: False\nroberta.encoder.layer.0.attention.output.dense.weight: False\nroberta.encoder.layer.0.attention.output.dense.bias: False\nroberta.encoder.layer.0.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.0.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.0.intermediate.dense.weight: False\nroberta.encoder.layer.0.intermediate.dense.bias: False\nroberta.encoder.layer.0.output.dense.weight: False\nroberta.encoder.layer.0.output.dense.bias: False\nroberta.encoder.layer.0.output.LayerNorm.weight: False\nroberta.encoder.layer.0.output.LayerNorm.bias: False\nroberta.encoder.layer.1.attention.self.query.weight: False\nroberta.encoder.layer.1.attention.self.query.bias: False\nroberta.encoder.layer.1.attention.self.key.weight: False\nroberta.encoder.layer.1.attention.self.key.bias: False\nroberta.encoder.layer.1.attention.self.value.weight: False\nroberta.encoder.layer.1.attention.self.value.bias: False\nroberta.encoder.layer.1.attention.output.dense.weight: False\nroberta.encoder.layer.1.attention.output.dense.bias: False\nroberta.encoder.layer.1.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.1.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.1.intermediate.dense.weight: False\nroberta.encoder.layer.1.intermediate.dense.bias: False\nroberta.encoder.layer.1.output.dense.weight: False\nroberta.encoder.layer.1.output.dense.bias: False\nroberta.encoder.layer.1.output.LayerNorm.weight: False\nroberta.encoder.layer.1.output.LayerNorm.bias: False\nroberta.encoder.layer.2.attention.self.query.weight: False\nroberta.encoder.layer.2.attention.self.query.bias: False\nroberta.encoder.layer.2.attention.self.key.weight: False\nroberta.encoder.layer.2.attention.self.key.bias: False\nroberta.encoder.layer.2.attention.self.value.weight: False\nroberta.encoder.layer.2.attention.self.value.bias: False\nroberta.encoder.layer.2.attention.output.dense.weight: False\nroberta.encoder.layer.2.attention.output.dense.bias: False\nroberta.encoder.layer.2.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.2.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.2.intermediate.dense.weight: False\nroberta.encoder.layer.2.intermediate.dense.bias: False\nroberta.encoder.layer.2.output.dense.weight: False\nroberta.encoder.layer.2.output.dense.bias: False\nroberta.encoder.layer.2.output.LayerNorm.weight: False\nroberta.encoder.layer.2.output.LayerNorm.bias: False\nroberta.encoder.layer.3.attention.self.query.weight: False\nroberta.encoder.layer.3.attention.self.query.bias: False\nroberta.encoder.layer.3.attention.self.key.weight: False\nroberta.encoder.layer.3.attention.self.key.bias: False\nroberta.encoder.layer.3.attention.self.value.weight: False\nroberta.encoder.layer.3.attention.self.value.bias: False\nroberta.encoder.layer.3.attention.output.dense.weight: False\nroberta.encoder.layer.3.attention.output.dense.bias: False\nroberta.encoder.layer.3.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.3.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.3.intermediate.dense.weight: False\nroberta.encoder.layer.3.intermediate.dense.bias: False\nroberta.encoder.layer.3.output.dense.weight: False\nroberta.encoder.layer.3.output.dense.bias: False\nroberta.encoder.layer.3.output.LayerNorm.weight: False\nroberta.encoder.layer.3.output.LayerNorm.bias: False\nroberta.encoder.layer.4.attention.self.query.weight: False\nroberta.encoder.layer.4.attention.self.query.bias: False\nroberta.encoder.layer.4.attention.self.key.weight: False\nroberta.encoder.layer.4.attention.self.key.bias: False\nroberta.encoder.layer.4.attention.self.value.weight: False\nroberta.encoder.layer.4.attention.self.value.bias: False\nroberta.encoder.layer.4.attention.output.dense.weight: False\nroberta.encoder.layer.4.attention.output.dense.bias: False\nroberta.encoder.layer.4.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.4.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.4.intermediate.dense.weight: False\nroberta.encoder.layer.4.intermediate.dense.bias: False\nroberta.encoder.layer.4.output.dense.weight: False\nroberta.encoder.layer.4.output.dense.bias: False\nroberta.encoder.layer.4.output.LayerNorm.weight: False\nroberta.encoder.layer.4.output.LayerNorm.bias: False\nroberta.encoder.layer.5.attention.self.query.weight: False\nroberta.encoder.layer.5.attention.self.query.bias: False\nroberta.encoder.layer.5.attention.self.key.weight: False\nroberta.encoder.layer.5.attention.self.key.bias: False\nroberta.encoder.layer.5.attention.self.value.weight: False\nroberta.encoder.layer.5.attention.self.value.bias: False\nroberta.encoder.layer.5.attention.output.dense.weight: False\nroberta.encoder.layer.5.attention.output.dense.bias: False\nroberta.encoder.layer.5.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.5.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.5.intermediate.dense.weight: False\nroberta.encoder.layer.5.intermediate.dense.bias: False\nroberta.encoder.layer.5.output.dense.weight: False\nroberta.encoder.layer.5.output.dense.bias: False\nroberta.encoder.layer.5.output.LayerNorm.weight: False\nroberta.encoder.layer.5.output.LayerNorm.bias: False\nroberta.encoder.layer.6.attention.self.query.weight: False\nroberta.encoder.layer.6.attention.self.query.bias: False\nroberta.encoder.layer.6.attention.self.key.weight: False\nroberta.encoder.layer.6.attention.self.key.bias: False\nroberta.encoder.layer.6.attention.self.value.weight: False\nroberta.encoder.layer.6.attention.self.value.bias: False\nroberta.encoder.layer.6.attention.output.dense.weight: False\nroberta.encoder.layer.6.attention.output.dense.bias: False\nroberta.encoder.layer.6.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.6.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.6.intermediate.dense.weight: False\nroberta.encoder.layer.6.intermediate.dense.bias: False\nroberta.encoder.layer.6.output.dense.weight: False\nroberta.encoder.layer.6.output.dense.bias: False\nroberta.encoder.layer.6.output.LayerNorm.weight: False\nroberta.encoder.layer.6.output.LayerNorm.bias: False\nroberta.encoder.layer.7.attention.self.query.weight: False\nroberta.encoder.layer.7.attention.self.query.bias: False\nroberta.encoder.layer.7.attention.self.key.weight: False\nroberta.encoder.layer.7.attention.self.key.bias: False\nroberta.encoder.layer.7.attention.self.value.weight: False\nroberta.encoder.layer.7.attention.self.value.bias: False\nroberta.encoder.layer.7.attention.output.dense.weight: False\nroberta.encoder.layer.7.attention.output.dense.bias: False\nroberta.encoder.layer.7.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.7.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.7.intermediate.dense.weight: False\nroberta.encoder.layer.7.intermediate.dense.bias: False\nroberta.encoder.layer.7.output.dense.weight: False\nroberta.encoder.layer.7.output.dense.bias: False\nroberta.encoder.layer.7.output.LayerNorm.weight: False\nroberta.encoder.layer.7.output.LayerNorm.bias: False\nroberta.encoder.layer.8.attention.self.query.weight: False\nroberta.encoder.layer.8.attention.self.query.bias: False\nroberta.encoder.layer.8.attention.self.key.weight: False\nroberta.encoder.layer.8.attention.self.key.bias: False\nroberta.encoder.layer.8.attention.self.value.weight: False\nroberta.encoder.layer.8.attention.self.value.bias: False\nroberta.encoder.layer.8.attention.output.dense.weight: False\nroberta.encoder.layer.8.attention.output.dense.bias: False\nroberta.encoder.layer.8.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.8.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.8.intermediate.dense.weight: False\nroberta.encoder.layer.8.intermediate.dense.bias: False\nroberta.encoder.layer.8.output.dense.weight: False\nroberta.encoder.layer.8.output.dense.bias: False\nroberta.encoder.layer.8.output.LayerNorm.weight: False\nroberta.encoder.layer.8.output.LayerNorm.bias: False\nroberta.encoder.layer.9.attention.self.query.weight: False\nroberta.encoder.layer.9.attention.self.query.bias: False\nroberta.encoder.layer.9.attention.self.key.weight: False\nroberta.encoder.layer.9.attention.self.key.bias: False\nroberta.encoder.layer.9.attention.self.value.weight: False\nroberta.encoder.layer.9.attention.self.value.bias: False\nroberta.encoder.layer.9.attention.output.dense.weight: False\nroberta.encoder.layer.9.attention.output.dense.bias: False\nroberta.encoder.layer.9.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.9.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.9.intermediate.dense.weight: False\nroberta.encoder.layer.9.intermediate.dense.bias: False\nroberta.encoder.layer.9.output.dense.weight: False\nroberta.encoder.layer.9.output.dense.bias: False\nroberta.encoder.layer.9.output.LayerNorm.weight: False\nroberta.encoder.layer.9.output.LayerNorm.bias: False\nroberta.encoder.layer.10.attention.self.query.weight: False\nroberta.encoder.layer.10.attention.self.query.bias: False\nroberta.encoder.layer.10.attention.self.key.weight: False\nroberta.encoder.layer.10.attention.self.key.bias: False\nroberta.encoder.layer.10.attention.self.value.weight: False\nroberta.encoder.layer.10.attention.self.value.bias: False\nroberta.encoder.layer.10.attention.output.dense.weight: False\nroberta.encoder.layer.10.attention.output.dense.bias: False\nroberta.encoder.layer.10.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.10.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.10.intermediate.dense.weight: False\nroberta.encoder.layer.10.intermediate.dense.bias: False\nroberta.encoder.layer.10.output.dense.weight: False\nroberta.encoder.layer.10.output.dense.bias: False\nroberta.encoder.layer.10.output.LayerNorm.weight: False\nroberta.encoder.layer.10.output.LayerNorm.bias: False\nroberta.encoder.layer.11.attention.self.query.weight: False\nroberta.encoder.layer.11.attention.self.query.bias: False\nroberta.encoder.layer.11.attention.self.key.weight: False\nroberta.encoder.layer.11.attention.self.key.bias: False\nroberta.encoder.layer.11.attention.self.value.weight: False\nroberta.encoder.layer.11.attention.self.value.bias: False\nroberta.encoder.layer.11.attention.output.dense.weight: False\nroberta.encoder.layer.11.attention.output.dense.bias: False\nroberta.encoder.layer.11.attention.output.LayerNorm.weight: False\nroberta.encoder.layer.11.attention.output.LayerNorm.bias: False\nroberta.encoder.layer.11.intermediate.dense.weight: False\nroberta.encoder.layer.11.intermediate.dense.bias: False\nroberta.encoder.layer.11.output.dense.weight: False\nroberta.encoder.layer.11.output.dense.bias: False\nroberta.encoder.layer.11.output.LayerNorm.weight: False\nroberta.encoder.layer.11.output.LayerNorm.bias: False\nclassifier.dense.weight: True\nclassifier.dense.bias: True\nclassifier.out_proj.weight: True\nclassifier.out_proj.bias: True\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:28:53.158336Z","iopub.execute_input":"2024-05-11T13:28:53.159028Z","iopub.status.idle":"2024-05-11T13:28:53.164760Z","shell.execute_reply.started":"2024-05-11T13:28:53.158983Z","shell.execute_reply":"2024-05-11T13:28:53.163812Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"AdamW (\nParameter Group 0\n    betas: (0.9, 0.999)\n    correct_bias: True\n    eps: 1e-06\n    lr: 0.001\n    weight_decay: 0.0\n)"},"metadata":{}}]},{"cell_type":"code","source":"num_epochs = 15\nfor epoch in range(num_epochs):\n    train(new_model, optimizer, criterion2, train_loader, 1)\n    all_preds,all_labels = evaluate(new_model, eval_loader)\n    all_preds = torch.tensor(all_preds).to(torch.float32)\n    all_labels = torch.tensor(all_labels).to(torch.float32)\n    loss =torch.sqrt(criterion2(all_preds,all_labels))\n    L1_loss =criterion3(all_preds,all_labels)\n    print('l1 loss: ',L1_loss)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Test Loss: {loss:.4f}\")\n    test_loss.append(loss)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:29:12.408746Z","iopub.execute_input":"2024-05-11T13:29:12.409573Z","iopub.status.idle":"2024-05-11T13:37:51.766011Z","shell.execute_reply.started":"2024-05-11T13:29:12.409532Z","shell.execute_reply":"2024-05-11T13:37:51.764764Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 182.6813\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:48<00:00,  1.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"l1 loss:  tensor(49.6059)\nEpoch 1/15, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 186.0201\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:47<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"l1 loss:  tensor(49.6059)\nEpoch 2/15, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 184.4204\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:47<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"l1 loss:  tensor(49.6059)\nEpoch 3/15, Test Loss: 75.4671\n","output_type":"stream"},{"name":"stderr","text":"                                                \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     all_preds,all_labels \u001b[38;5;241m=\u001b[39m evaluate(new_model, eval_loader)\n\u001b[1;32m      5\u001b[0m     all_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(all_preds)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n","Cell \u001b[0;32mIn[40], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 21\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m input_ids, attention_mask, labels, outputs\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Save the trained model\noutput_dir = \"/kaggle/working/\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(\"Model saved successfully.\")\n\n# Load the saved model\nloaded_model = XLMRobertaForSequenceClassification.from_pretrained(output_dir)\nloaded_tokenizer = XLMRobertaTokenizer.from_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:38:50.291378Z","iopub.execute_input":"2024-05-11T13:38:50.291742Z","iopub.status.idle":"2024-05-11T13:38:54.468090Z","shell.execute_reply.started":"2024-05-11T13:38:50.291714Z","shell.execute_reply":"2024-05-11T13:38:54.467307Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Model saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}